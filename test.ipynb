{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SupervisionWarnings: annotate is deprecated: `BoxAnnotator` is deprecated and will be removed in `supervision-0.22.0`. Use `BoundingBoxAnnotator` and `LabelAnnotator` instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict, annotate, batch_predict\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "model = load_model(\"groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"groundingdino_swint_ogc.pth\")\n",
    "IMAGE_PATH = \"/Users/derek/Desktop/Drone-detection-dataset/output_frames/V_DRONE_001/frame_0000.jpg\"\n",
    "IMAGE_PATH = \"/Users/derek/Desktop/multiple_planes.jpg\"\n",
    "TEXT_PROMPT = \"flying object\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "image_source, image = load_image(IMAGE_PATH)\n",
    "\n",
    "# boxes, logits, phrases = predict(\n",
    "#     model=model,\n",
    "#     # preprocessed_images=image.unsqueeze(0),\n",
    "#     image=image,\n",
    "#     caption=TEXT_PROMPT,\n",
    "#     box_threshold=BOX_TRESHOLD,\n",
    "#     text_threshold=TEXT_TRESHOLD,\n",
    "#     device=\"cpu\"\n",
    "# )\n",
    "images = torch.stack([image, image])\n",
    "boxes, logits, boxes_to_im = batch_predict(\n",
    "    model=model,\n",
    "    preprocessed_images=images,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_TRESHOLD,\n",
    "    text_threshold=TEXT_TRESHOLD,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=[\"object\"])\n",
    "cv2.imwrite(\"annotated_image.jpg\", annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes_to_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "UserWarning: Failed to load custom C++ ops. Running on CPU mode Only!\n",
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3588.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0107e+02, 3.7061e-01, 4.9936e+01, 1.5729e-01]]) 3 800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Detections(xyxy=array([[176.09795   ,   0.2919684 , 226.03407   ,   0.44925904]],\n",
       "       dtype=float32), mask=None, confidence=array([0.7914222], dtype=float32), class_id=None, tracker_id=None, data={}),\n",
       " ['object'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from groundingdino.util.inference import Model\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    model_config_path=\"groundingdino/config/GroundingDINO_SwinT_OGC.py\",\n",
    "    model_checkpoint_path=\"groundingdino_swint_ogc.pth\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "IMAGE_PATH = \"/Users/derek/Desktop/Drone-detection-dataset/output_frames/V_DRONE_001/frame_0000.jpg\"\n",
    "TEXT_PROMPT = \"flying object\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "image = Model.preprocess_image_path(IMAGE_PATH)\n",
    "\n",
    "\n",
    "model.predict_with_caption(\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_TRESHOLD,\n",
    "    text_threshold=TEXT_TRESHOLD,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7993, -0.7993, -0.7993,  ..., -0.8849, -0.8849, -0.8849],\n",
       "         [-0.7993, -0.7993, -0.7993,  ..., -0.8849, -0.8849, -0.8849],\n",
       "         [-0.7993, -0.7993, -0.7993,  ..., -0.8849, -0.8849, -0.8849],\n",
       "         ...,\n",
       "         [-0.9877, -0.9705, -0.9534,  ..., -1.1075, -1.1418, -1.1589],\n",
       "         [-1.0048, -0.9877, -0.9705,  ..., -1.1247, -1.1075, -1.0733],\n",
       "         [-1.0048, -0.9877, -0.9705,  ..., -1.1247, -1.0733, -1.0219]],\n",
       "\n",
       "        [[ 0.1176,  0.1176,  0.1176,  ..., -0.0574, -0.0574, -0.0574],\n",
       "         [ 0.1176,  0.1176,  0.1176,  ..., -0.0574, -0.0574, -0.0574],\n",
       "         [ 0.1176,  0.1176,  0.1176,  ..., -0.0574, -0.0574, -0.0574],\n",
       "         ...,\n",
       "         [-0.6176, -0.6001, -0.5826,  ..., -0.8277, -0.8627, -0.8803],\n",
       "         [-0.6352, -0.6176, -0.6001,  ..., -0.8452, -0.8277, -0.7927],\n",
       "         [-0.6352, -0.6176, -0.6001,  ..., -0.8452, -0.7927, -0.7402]],\n",
       "\n",
       "        [[ 1.2457,  1.2457,  1.2457,  ...,  1.0714,  1.0714,  1.0714],\n",
       "         [ 1.2457,  1.2457,  1.2457,  ...,  1.0714,  1.0714,  1.0714],\n",
       "         [ 1.2457,  1.2457,  1.2457,  ...,  1.0714,  1.0714,  1.0714],\n",
       "         ...,\n",
       "         [-0.9330, -0.9156, -0.8981,  ..., -1.0201, -1.0550, -1.0724],\n",
       "         [-0.9504, -0.9330, -0.9156,  ..., -1.0376, -1.0201, -0.9853],\n",
       "         [-0.9504, -0.9330, -0.9156,  ..., -1.0376, -0.9853, -0.9330]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7993, -0.7993, -0.7993,  ..., -0.8849, -0.8849, -0.8849],\n",
       "         [-0.7993, -0.7993, -0.7993,  ..., -0.8849, -0.8849, -0.8849],\n",
       "         [-0.7993, -0.7993, -0.7993,  ..., -0.8849, -0.8849, -0.8849],\n",
       "         ...,\n",
       "         [-0.9877, -0.9705, -0.9534,  ..., -1.1075, -1.1418, -1.1589],\n",
       "         [-1.0048, -0.9877, -0.9705,  ..., -1.1247, -1.1075, -1.0733],\n",
       "         [-1.0048, -0.9877, -0.9705,  ..., -1.1247, -1.0733, -1.0219]],\n",
       "\n",
       "        [[ 0.1176,  0.1176,  0.1176,  ..., -0.0574, -0.0574, -0.0574],\n",
       "         [ 0.1176,  0.1176,  0.1176,  ..., -0.0574, -0.0574, -0.0574],\n",
       "         [ 0.1176,  0.1176,  0.1176,  ..., -0.0574, -0.0574, -0.0574],\n",
       "         ...,\n",
       "         [-0.6176, -0.6001, -0.5826,  ..., -0.8277, -0.8627, -0.8803],\n",
       "         [-0.6352, -0.6176, -0.6001,  ..., -0.8452, -0.8277, -0.7927],\n",
       "         [-0.6352, -0.6176, -0.6001,  ..., -0.8452, -0.7927, -0.7402]],\n",
       "\n",
       "        [[ 1.2457,  1.2457,  1.2457,  ...,  1.0714,  1.0714,  1.0714],\n",
       "         [ 1.2457,  1.2457,  1.2457,  ...,  1.0714,  1.0714,  1.0714],\n",
       "         [ 1.2457,  1.2457,  1.2457,  ...,  1.0714,  1.0714,  1.0714],\n",
       "         ...,\n",
       "         [-0.9330, -0.9156, -0.8981,  ..., -1.0201, -1.0550, -1.0724],\n",
       "         [-0.9504, -0.9330, -0.9156,  ..., -1.0376, -1.0201, -0.9853],\n",
       "         [-0.9504, -0.9330, -0.9156,  ..., -1.0376, -0.9853, -0.9330]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
